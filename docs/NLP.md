# Deep learning basic

**Table of contents**
- [자연어 처리를 위한 패키지 설치](#자연어-처리를-위한-패키지-설치)
- [지도 학습](#지도-학습)
- [통계적 언어 모델](#통계적-언어-모델)
- [N-gram 언어 모델](#엔그램-언어-모델)
- [단어 표현방식](#단어-표현-방식)
- [텍스트 전처리 과정 1. 토큰화](#텍스트-토큰화)
- [텍스트 전처리 과정 2. 정제와 정규화](#텍스트-정제와-정규화)
- [텍스트 전처리 과정 3. 정수인코딩](#정수-인코딩)
- [벡터 데이터 유사도](#벡터-데이터-유사도)
- [Bag of Words](#bag-of-words)
- [워드 임베딩](#워드-임베딩)
- [희소 표현](#희소-표현)
- [밀집 표현](#밀집-표현)
- [워드투벡터](#word2vector)

## 자연어 처리를 위한 패키지 설치
자연어 (NL)란 우리가 일상 생활에서 사용하는 언어를 의미하며, 자연어 처리 (NLP)란 이런 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일을 의미하며, 챗봇, 감성 분석 및 뉴스 기사 분류 등과 같은 분야에서 이용 되고 있다.

### 텐서 플로우 설치
파이썬 버전과 테스트된 빌드는 [다음 표](https://www.tensorflow.org/install/source?hl=ko#tested_build_configurations)를 참고한다. 아래와 같이 설치를 진행한다.

```bash
pip install tensorflow
```
### 케라스 설치
Keras는 딥 러닝 프레임워크인 텐서 플로우에 대한 추상화 된 API를 제공한다. 앞서 텐서플러우를 설치하면 함께 설치 된다.

```bash
pip install keras
```

### Gensim 설치
젠심은 머신 러닝을 사용하여 토픽 모델링과 자연어처리 등을 수행할 수 있게 해주는 오픈 소스 라이브러리이다.

```bash
pip install gensim
```
### 사이킷런
Scikit-learn은 파이썬 머신러닝 라이브러리이다. 사이킷런에는 머신러닝을 연습하기 위한 아이리스 데이터, 당뇨병 데이터 등 자체 데이터를 포함하고 있다.

```bash
pip install scikit-learn
```

### NLTK 설치
텍스트 전처리를 위한 패키지를 설치한다.

```bash
pip install nltk
```
그리고, 파이썬 쉘에서 다음 코드를 입력하여 NLTK 데이터를 모두 다운로드 받는다.

```bash
import nltk
nltk.download()
```

### KoNLPy 설치
한국어 자연어 처리를 위해 아래와 같이 설치한다.

```bash
pip install konlpy
```

추가적으로 아래와 같이 데이터 분석을 위한 패키지를 설치한다.

```bash
pip install pandas numpy matplotlib
```

## 지도 학습
지도학습 (Supervised Learning)을 하기 위해서는 훈련 데이터가 필요하다. 훈련 데이터는 정답이 적혀 있는 문제집과 같다. 기계는 정답이 적혀져 있는 문제지를 제공 받아 문제와 정답을 함께 보면서 열심히 공부하고, 향후에 정답이 없는 문제에 대해서도 정답을 잘 예측해야 한다. 예를 들어, 스팸메일을 분류하기 위한 훈련데이터를 문제지 데이터와 정답 데이터로 분리해야 한다. 그리고 훈련데이터의 일부를 테스트 데이터로 분리 할 수 있다.

## 통계적 언어 모델
언어 모델 (Language Model)은 언어라는 현상을 모델링하고자 단어 시퀀스(문장)에 확률을 할당(assign)하는 모델을 말한다. 언어 모델을 만드는 방법은 크게는 **통계를 이용한 방법**과 **인공 신경망을 이용한 방법**으로 구분할 수 있다. 최근에는 통계를 이용한 방법보다는 인공 신경망을 이용한 방법이 더 좋은 성능을 보여주고 있다. 최근 핫한 자연어 처리의 기술인 GPT나 BERT 또한 인공 신경망 언어 모델의 개념을 사용하여 만들어졌다. 전통적 통계적 언어 모델 (Statistical Language Model)은 단어 시퀀스에 확률을 할당하는 일을 한다. 즉 이전 단어들이 주어졌을 때 다음 단어를 예측하여 가장 자연스러운 단어 시퀀스를 찾아낸다. 예를 들어, *비행기를 타려고 공항에 갔는데 차가 너무 막히는 바람에 비행기를* 라는 문장이 있을때 사람은 쉽게 *놓쳤다*라고 생각한다. 마찬가지로 기계도 앞선 단어를 보고 *놓쳤다* 라는 단어가 가장 자연스러운 단어라는 것을 찾게 하는 것을 통계적 언어 모델링이라고 한다. SLM은 기계가 학습한 코퍼스 데이터에서 "차가 너무 막히는 바람에 비행기를" 가 10000번 등장했는데 그 다음에 *놓쳤다*가 3000번 나왔다면 30%의 확률을 가지고 있다고 보고 가장 큰 확률값에 의해 그 다음 단어를 도출한다. 이 때 만약 충분한 데이터를 갖고 있지 못하다면 언어를 정확히 모델링하지 못하는 문제가 발생하는데 희소성의 문제 (sparsity problem)이라고 한다.

## 엔그램 언어 모델
N-gram언어 모델은 앞서 설명한 SLM의 리종이지만 앞선 모든 단어를 고려하는 것이 아닌 일부 단어만 고려하는 방식이며 여기서 N이 가지는 의미는 *일부 단어 몇 개를 보느냐*를 말한다. 예를 들어, *My lovely wife is smiling*이라는 문장에서 *My lovely wife* 다음에 *is*가 나올 확률을 기반으로 해당 단어를 도출한다. 그러나 커퍼스 데이터에서 *My lovely wife* 다음에 *is*가 나올 확률 보다 *wifi* 다음에 *is*가 나올 확률이 더 많다면 전체 단어를 활용하는 것 보다 *wifi*라는 단어 만으로 확률을 따져 보는 것이 더 낳을 수 있다는 것이 N-gram언어 모델의 기본 시작점이다. 다른 예를 들어 N의 의미를 설명하자면, 다음과 같은 문장이 있을 때 n-gram을 전부 구해 보면 다음과 같다.

*An adorable little boy is spreading smiles*

- uni-grams: an, adorable, little, boy, is, spreading, smiles
- bi-grams: an adorable, adorable little, little boy, boy is, is spreading, spreading smiles
- tri-grams: an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles
- 4-grams: an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles

N-gram 언어 모델에서는 다음에 나올 단어 예측을 오직 N-1개의 단어만 가지고 한다. 예를 들어, *An adorable little boy is spreading* 다음에 나올 단어를 찾기 위해 4-grams 모델은 앞에 3개의 단어 즉, *boy is spreading* 단어만을 고려한다. 여기서 만약, 코퍼스 데이터에 *boy is spreading* 다음에 *insult*가 5000번 나오고, *smile*가 300번 나온다면 그 다음 단어로 확률적으로 insult를 선택한다. 이것이 N-gram 모델의 한계이다.

## 텍스트 토큰화
Corpus 데이터는 training 자료로 사용되지 전에 토큰화-정제-정규화 과정이라는 전처리 과정을 거친다. 

### Word tokenization
토큰화 작업을 단순하게 코퍼스에서 구두점을 제외하고 공백 기준으로 잘라내는 작업이라고 볼 수 없다. 구두점이나 특수 문자를 단순하게 제외하면 단어와 문장의 의미를 잃을 수 있다. 예를 들어,
23.55 달러와 같은 가격을 단순히 구두점(.)으로 분리하면 그 본래의 의미를 잃게 된다. 그리고, 340,000,000원과 같이 숫자 사이에ㅔ 콤마(,)가 들어가는 경우에도 단순히 콤마를 제거해서는 안된다.

### Setence tokenization
토큰의 단위가 문장(sentence)일 경우, 이 작업은 갖고있는 코퍼스 내에서 문장 단위로 구분하는 작업으로 때로는 문장 분류(sentence segmentation)라고도 부른다. 보통 갖고있는 코퍼스가 정제되지 않은 상태라면, 코퍼스는 문장 단위로 구분되어 있지 않아서 이를 사용하고자 하는 용도에 맞게 문장 토큰화가 필요할 수 있다. 문장 토큰화도 간단하게 생각했을 때, 마침표, 물음표 혹은 느낌표 등으로 문장을 분리할 수 있다. 하지만, 아래와 같이 문장 내에 마침표 등이 들어가게 되면 제대로 된 문장으로 잘라낼 수 없게 된다.

```bash
IP 주소가 192.168.43.56 이고, 아이디는 너잘랐어 이고, 비밀번호는 네똥굻다를 영어로 입력하면되.
```

특히, 한국어는 영어와 달리 교착어라는 특성을 가지기 때문에 단순히 띄어쓰기나 마침표 등으로 문장을 분리할 수 없다. 교착어란 조사, 어미 등을 붙여서 말을 만드는 언어를 의미한다. 예를 들어, 영어에 "그" 라는 의미의 he/him이 포함된 경우 단순히 띄어쓰기로 분리해도 그 의미를 파악하는데 문제가 없다. 하지만, 한국어에는 '그가', '그에게', '그를', '그와', '그는'과 같이 다양한 조사가 '그'라는 글자 뒤에 띄어쓰기 없이 바로 붙게된다. 따라서 띄어쓰기 단위가 영어처럼 독립적인 단어라면 띄어쓰기 단위로 토큰화를 하면 되지만 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등이 뒤에 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다.

한국어 토큰화에서는 형태소(morpheme) 란 개념을 반드시 이해해야 한다. 형태소(morpheme)란 뜻을 가진 가장 작은 말의 단위를 말하며 자립 형태소와 의존 형태소로 나누어 진다.

- 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.
- 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간을 말한다.

한국어에서 영어에서의 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아니라 형태소 토큰화를 수행해야한다.

## 텍스트 정제와 정규화
코퍼스에서 용도에 맞게 토큰을 분류하는 작업을 토큰화(tokenization)라고 하며, 토큰화 작업 전, 후에는 텍스트 데이터를 용도에 맞게 정제(cleaning) 및 정규화(normalization)가 필요하다.

- 정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거
- 정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어주는 과정

정제 작업은 아래와 같은 작업을 의미한다. 자연어 처리에 크게 도움이 되지 않는 데이터를 노이즈 데이터라고 하며 노이즈를 제거하는 작업을 의미한다.

- 등장 빈도가 적은 단어 제거
- 길이가 짧은 단어 제거
- 불용어 제거

특히 커퍼스에서 큰 의미가 없는 단어를 불용어(stopword)라고 하는데, at, in 등과 같은 조사나 접미사가 그에 해당한다. 자연어 처리를 위한 파이썬 패키지인 NLTK에서는 약 100여개 이상의 영어 단어들을 불용어로 미리 정의하고 있으며 그 외 사용자가 직접 불용어를 추가 할 수 있다.

정규화 방법으로는 어간 추출(Stemming)과 표제어 추출(Lemmatization)이 있다. 이러한 방법을 이용하여 결국 표기는 다르지만 한나의 단어로 일반화 시킬 수 있다면 하나의 단어로 통일하여 문서 내의 단어 수를 줄이고자 하는 것이다.

## 정수 인코딩
정수 인코딩은 임베딩의 기초가 되는 기법으로 자연어에 정수를 맵핑하는 것을 의미한다. 즉, 단어에 인덱스를 부여하여 컴퓨터가 텍스트보다 숫자를 더 잘 처리한다는 원리를 이용한다. 예를 들어, 단어를 빈도수 순으로 정렬하여 빈도수가 높은 순서대로 차례로 낮은 숫자부터 정수를 부여할 수 있다.

## 단어 표현 방식
단어를 숫자로 표현하는 방식에는 크게 local representation 방식과 ditributed representation 방식으로 나누어진다. 예를 들어, lovely, cute, wife라는 단어를 두고 단순히 단어 하나하나에 특정 값을 부여하는 방식을 local representation이라고 하며, wife주변에 등장하는 lovely, cute 라는 단어를 연관지어 값을 부여하는 방식을 ditributed representation이라고 한다. 

## Bag of Words
단어 표현방식 중 local representation에 해당하는 BoW 방식은 단어의 등장 순서를 고져라지 않는 빈도수 기반의 표현방식이다. 즉,단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법이다. 단어가 있는 가방(Bag of Words)에는 특정 단어가 N개가 있을 수 있다. 또한 가방안의 단어의 순서는 중요하지 않다.

## 워드 임베딩
텍스트를 컴퓨터가 이해하고, 효율적으로 처리하게 하기 위해서는 컴퓨터가 이해할 수 있도록 텍스트를 적절히 숫자로 변환해야 한다. 단어를 표현하는 방법에 따라서 자연어 처리의 성능이 크게 달라지기 때문에 단어를 수치화 하기 위한 많은 연구가 있었고, 현재에 이르러서는 각 단어를 인공 신경망 학습을 통해 벡터화하는 워드 임베딩이라는 방법이 가장 많이 사용되고 있다.

## 희소 표현
앞서 원-핫 인코딩을 통해서 나온 원-핫 벡터들은 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되는 벡터 표현 방법이다. 이렇게 벡터 또는 행렬(matrix)의 값이 대부분이 0으로 표현되는 방법을 희소 표현(sparse representation)이라고 한다. 따라서 원-핫 벡터는 희소 벡터(sparse vector)에 해당된다. 이러한 희소 벡터의 문제점은 공간의 낭비가 너무 심하다는 것이다. 단어의 갯수가 늘어나면 벡터의 차원도 늘어 나게 된다.

## 밀집 표현
밀집 표현(dense representation)은 희소 표현과 반대되는 개념이다. 밀집 표현은 벡터의 차원을 단어의 집합 크기로 나타내는 것이 아니며 사용자가 설정한 값으로 모든 단어의 벡터 표현의 차원을 맞춘다. 만약 사용자가 밀집 표현의 차원을 64로 설정한다면 모든 단어의 벡터 표현은 64차원에서 표현되며 모든 값이 0과 1 뿐만 아니라 실수로도 표현된다. 이러한 밀집 표현 방식으로 단어를 표현하는 방식을 워드 임베딩(word embedding)이라고 한다.그리고 워드 임베딩을 통해 나온 결과를 임베딩 벡터(embedding vector)라고 한다.

# Word2Vector
워드투벡은 분산 표현 (distributed representaion)의 일종이다. 즉, 단어 벡터 간 유의미한 유사도를 반영할 수 있도록 단어의 의미를 수치화하여 벡터로 표현하는 기법이다. 앞서 원-핫 인코딩을 통해서 얻은 원-핫 벡터는 표현하고자 하는 단어의 인덱스의 값만 1이고, 나머지 인덱스에는 전부 0으로 표현되어 벡터 또는 행렬의 값이 대부분이 0으로 표현되는 희소 표현(sparse representation) 기법을 이용한다. 하지만 이러한 표현 방법은 각 단어 벡터간 유의미한 유사성을 표현할 수 없다는 단점이 있어, 대안으로 단어의 의미를 다차원 공간에 벡터화하는 방법을 사용하는데 이러한 표현을 분산 표현(distributed representation) 이라고 한다. 그리고 분산 표현을 이용하여 단어 간 의미적 유사성을 벡터화하는 작업을 워드 임베딩(embedding)이라 부르며 이렇게 표현된 벡터를 임베딩 벡터(embedding vector)라고 한다. 유사한 단어들은 유사한 벡터 값을 가진다고 보면 된다.

## 벡터 데이터 유사도
벡터 유사도 (Vector Similiarity)